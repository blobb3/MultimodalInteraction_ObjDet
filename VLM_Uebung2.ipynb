{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python UEBUNGEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Zählen der Anzahl von Nachrichten in der Message - Liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Nachrichten: 3\n"
     ]
    }
   ],
   "source": [
    "# Beispiel: Nachrichtenanzahl in der Liste überprüfen\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"This is a system prompt.\"},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Describe the image\"}]},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is the background?\"}]}\n",
    "]\n",
    "\n",
    "# Anzahl der Nachrichten in der Liste\n",
    "print(\"Anzahl der Nachrichten:\", len(messages))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Länge des Textes in Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Länge des Prompts: 29\n"
     ]
    }
   ],
   "source": [
    "# Länge des Text-Prompts überprüfen\n",
    "prompt = \"Describe the image in detail.\"\n",
    "print(\"Länge des Prompts:\", len(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Untersuchen der Rückgabe eines Outputs mit Schlüsseln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Output simulieren\n",
    "output = {\n",
    "    \"description\": {\n",
    "        \"scene\": \"A busy street with cars and people.\",\n",
    "        \"foreground\": {\n",
    "            \"main_object\": \"car\",\n",
    "            \"secondary_objects\": [\"people\", \"bicycle\"]\n",
    "        },\n",
    "        \"background\": \"Buildings and trees.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Schlüssel im Output zählen\n",
    "print(\"Anzahl der Schlüssel in 'description':\", len(output[\"description\"]))\n",
    "print(\"Schlüssel in 'foreground':\", list(output[\"description\"][\"foreground\"].keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Iterieren durch eine Liste von JSON-Antworten und deren Länge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Antworten: 3\n",
      "Länge von Antwort 1: 10\n",
      "Länge von Antwort 2: 10\n",
      "Länge von Antwort 3: 10\n"
     ]
    }
   ],
   "source": [
    "# Beispiel: Mehrere JSON-Antworten simulieren\n",
    "json_responses = [\n",
    "    {\"text\": \"Response 1\", \"tokens\": 15},\n",
    "    {\"text\": \"Response 2\", \"tokens\": 20},\n",
    "    {\"text\": \"Response 3\", \"tokens\": 25}\n",
    "]\n",
    "\n",
    "# Länge der Liste der Antworten\n",
    "print(\"Anzahl der Antworten:\", len(json_responses))\n",
    "\n",
    "# Überprüfen der Länge der Texte in den Antworten\n",
    "for i, response in enumerate(json_responses, start=1):\n",
    "    print(f\"Länge von Antwort {i}:\", len(response[\"text\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Finden der längsten Antwort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Längste Antwort: A much longer response describing the scene.\n"
     ]
    }
   ],
   "source": [
    "# Liste von Antworten simulieren\n",
    "responses = [\"Short response\", \"A much longer response describing the scene.\", \"Medium length response\"]\n",
    "\n",
    "# Längste Antwort finden\n",
    "longest_response = max(responses, key=len)\n",
    "print(\"Längste Antwort:\", longest_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. JSON-Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Schlüssel im JSON: 3\n",
      "Länge der Antwort: 25\n"
     ]
    }
   ],
   "source": [
    "# Beispiel-JSON simulieren\n",
    "example_json = {\n",
    "    \"user\": \"test_user\",\n",
    "    \"model_used\": \"gpt-4o-mini\",\n",
    "    \"response\": {\"type\": \"text\", \"content\": \"This is the response text\"}\n",
    "}\n",
    "\n",
    "# Überprüfen der Gesamtschlüssel im JSON\n",
    "print(\"Anzahl der Schlüssel im JSON:\", len(example_json))\n",
    "\n",
    "# Länge der Antwort im Feld \"content\"\n",
    "response_length = len(example_json[\"response\"][\"content\"])\n",
    "print(\"Länge der Antwort:\", response_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umgang mit Fehler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    value = output[\"description\"][\"foreground\"][\"main_object\"]\n",
    "except KeyError:\n",
    "    value = \"Schlüssel nicht gefunden\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Beispielaufgaben für jedes Modell**\n",
    "\n",
    "#### **YOLO**\n",
    "1. **Aufgabe:** \"Erkenne alle Autos und Fußgänger auf diesem Bild.\"\n",
    "   - Input: Straßenbild.\n",
    "   - Output: Bounding Boxes für Autos und Fußgänger.\n",
    "2. **Aufgabe:** \"Zähle die Anzahl der Fahrzeuge in einem Video.\"\n",
    "   - Input: Verkehrsvideo.\n",
    "   - Output: Fahrzeuganzahl pro Frame.\n",
    "\n",
    "#### **OWL-ViT**\n",
    "1. **Aufgabe:** \"Finde das Objekt, das aussieht wie ein 'alter Holztisch'.\"\n",
    "   - Input: Bild eines Raumes.\n",
    "   - Output: Bounding Box und Beschreibung des gefundenen Objekts.\n",
    "2. **Aufgabe:** \"Markiere alle ungewöhnlichen Objekte in diesem Bild.\"\n",
    "   - Input: Naturbild.\n",
    "   - Output: Bounding Boxes für seltene Objekte.\n",
    "\n",
    "#### **VLMs**\n",
    "1. **Aufgabe:** \"Was ist auf diesem Bild zu sehen? Beschreibe die Szene.\"\n",
    "   - Input: Bild eines Parks.\n",
    "   - Output: Textbeschreibung der Szene.\n",
    "2. **Aufgabe:** \"Welches Objekt im Bild passt zur Beschreibung 'eine kleine blaue Vase'?\"\n",
    "   - Input: Bild und Beschreibung.\n",
    "   - Output: Objektposition und Beschreibung.\n",
    "\n",
    "#### **GPT-4**\n",
    "1. **Aufgabe:** \"Extrahiere den gesamten Text aus diesem Bild.\"\n",
    "   - Input: Bild mit Text (z. B. Straßenschild oder Dokument).\n",
    "   - Output: Vollständiger Text im Bild.\n",
    "2. **Aufgabe:** \"Analysiere diesen Text und finde die Hauptpunkte.\"\n",
    "   - Input: Textdokument.\n",
    "   - Output: Zusammenfassung oder Hauptpunkte.\n",
    "\n",
    "#### **GEMINI**\n",
    "1. **Aufgabe:** \"Vergleiche die Anordnung der Objekte in diesen zwei Bildern.\"\n",
    "   - Input: Zwei Bilder von Innenräumen.\n",
    "   - Output: Textbeschreibung der Unterschiede.\n",
    "2. **Aufgabe:** \"Erstelle eine Liste aller sichtbaren Objekte und deren Positionen im Bild.\"\n",
    "   - Input: Bild eines Lagers.\n",
    "   - Output: Liste der Objekte mit Koordinaten.\n",
    "\n",
    "---\n",
    "\n",
    "### **Vergleich der Modelle**\n",
    "| Modell     | Hauptaufgabe                | Flexibilität     | Stärken                          | Schwächen                       |\n",
    "|------------|-----------------------------|------------------|-----------------------------------|----------------------------------|\n",
    "| YOLO       | Objekterkennung             | Eingeschränkt    | Schnell, Echtzeit, präzise       | Begrenztes Vokabular            |\n",
    "| OWL-ViT    | Offenes Objekterkennung     | Mittel           | Offenes Vokabular, flexibel      | Weniger schnell                 |\n",
    "| VLM        | Multimodale Aufgaben        | Hoch             | Kombination aus Text und Bild    | Abhängig von Datenqualität      |\n",
    "| GPT-4      | Textanalyse und Multimodal  | Sehr hoch        | Sehr flexibel, vielseitig        | Rechenintensiv                  |\n",
    "| GEMINI     | Tiefergehende Multimodalität| Sehr hoch        | Präzise für Bild-Text-Kombination| Rechenintensiv, experimentell   |\n",
    "\n",
    "---\n",
    "\n",
    "Wenn Sie weitere Details oder eine tiefere Analyse zu den Modellen benötigen, lassen Sie es mich wissen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Zusammenfassung der Modelle und ihre Einsatzgebiete**\n",
    "\n",
    "#### **1. YOLO (You Only Look Once)**\n",
    "- **Eignung:**\n",
    "  - Speziell für **Objekterkennung** in Bildern und Videos.\n",
    "  - Besonders effektiv für **echtzeitfähige Anwendungen**.\n",
    "  - Liefert präzise Bounding Boxes und Klassifizierungen für bekannte Objekte aus einem festgelegten Vokabular.\n",
    "- **Stärken:**\n",
    "  - Schnell und ressourceneffizient.\n",
    "  - Gut geeignet für Szenarien mit hoher Geschwindigkeit oder Echtzeitanalysen.\n",
    "- **Beispiele:**\n",
    "  - Überwachungskameras.\n",
    "  - Erkennung von Verkehrszeichen oder Fahrzeugen.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. OWL-ViT (Open Vocabulary Vision Transformer)**\n",
    "- **Eignung:**\n",
    "  - Erweiterte Objekterkennung mit einem offenen Vokabular.\n",
    "  - Ideal für **unbekannte Objekte**, die nicht in vorgegebenen Klassen enthalten sind.\n",
    "  - Kann Beschreibungen wie \"roter Stuhl\" oder \"grüner Baum\" verwenden.\n",
    "- **Stärken:**\n",
    "  - Flexibler als YOLO bei unbekannten Objekten.\n",
    "  - Gut geeignet für **semantische Analysen**, z. B. Szenenbeschreibungen oder seltene Klassen.\n",
    "- **Beispiele:**\n",
    "  - Analyse von neuen Produkten in einem Lager.\n",
    "  - Detektion von Objekten in unstrukturierten Umgebungen.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. VLMs (Vision-Language Models, z. B. GPT-4 Vision)**\n",
    "- **Eignung:**\n",
    "  - Kombinierte Analyse von Text und Bild.\n",
    "  - **Multimodale Aufgaben**, z. B. Textbeschreibungen basierend auf Bildinhalten oder Bilder basierend auf textuellen Anfragen.\n",
    "- **Stärken:**\n",
    "  - Flexibel in der Interpretation und Kombination von Bild- und Textinformationen.\n",
    "  - Gut geeignet für **Szenenanalyse, Beschreibungserstellung oder Dialoge** mit Bildinhalten.\n",
    "- **Beispiele:**\n",
    "  - \"Beschreibe, was auf diesem Bild zu sehen ist.\"\n",
    "  - \"Welche Objekte befinden sich im Vordergrund dieses Bildes?\"\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. GPT-4**\n",
    "- **Eignung:**\n",
    "  - Textbasierte Aufgaben wie **Sprache, Übersetzung, Textzusammenfassung**.\n",
    "  - Ergänzend auch für multimodale Aufgaben wie **Textextraktion aus Bildern**.\n",
    "- **Stärken:**\n",
    "  - Extrem flexibel bei textuellen Aufgaben.\n",
    "  - Unterstützt durch Plug-ins (z. B. GPT-4 Vision) auch visuelle Analysen.\n",
    "- **Beispiele:**\n",
    "  - \"Fasse diesen Text zusammen.\"\n",
    "  - \"Extrahiere alle Objekte aus diesem Bild und beschreibe sie.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. GEMINI**\n",
    "- **Eignung:**\n",
    "  - Kombination aus visuellem und sprachlichem Verständnis, ähnlich wie GPT-4 Vision.\n",
    "  - **Erweiterte multimodale Aufgaben** mit höherem Fokus auf detaillierte Analysen.\n",
    "- **Stärken:**\n",
    "  - Fortgeschrittene Interaktion zwischen visuellen und sprachlichen Informationen.\n",
    "  - Gut geeignet für **Vergleichsaufgaben** oder tiefgehende Analysen.\n",
    "- **Beispiele:**\n",
    "  - \"Vergleiche die Objekte in diesen beiden Bildern.\"\n",
    "  - \"Wie unterscheidet sich der Stil dieser beiden Kunstwerke?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
