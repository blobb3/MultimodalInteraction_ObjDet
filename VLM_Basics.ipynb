{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. OpenAI VLM (GPT-4*) - Basics\n",
    "This section demonstrates the basic usage of OpenAI's Vision Language Model (VLM) capabilities using GPT-4.\n",
    "We will use the OpenAI API to analyze an image and provide detailed textual insights.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation \n",
    "- https://platform.openai.com/docs/guides/vision?lang=node\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=image \n",
    "- https://platform.openai.com/docs/api-reference/chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "#openAIclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "openAIclient = openai.OpenAI(api_key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TEXTMODEL = \"gpt-4o-mini\" \n",
    "IMGMODEL= \"gpt-4o-mini\" \n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The image depicts a bustling urban street scene. Key elements include:\\n\\n- A city\n",
      "environment with tall buildings and shops.\\n- People engaging in various activities: a boy sitting on the ground with a\n",
      "device, a woman reading on a bench, and a man playing guitar.\\n- A cyclist in motion, a person on a scooter, and\n",
      "pedestrians crossing the street.\\n- Pigeons scattered near a flower pot on the ground.\\n- It appears to be a lively\n",
      "moment captured in a city, possibly depicting the vibrancy of urban life.', role='assistant', function_call=None,\n",
      "tool_calls=None, refusal=None)\n"
     ]
    }
   ],
   "source": [
    "#basic call to gpt4 with prompt and image\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message)\n",
    "print(textwrap.fill(response, width=120))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1 Structured Output\n",
    "Here, we expand upon the VLM example to request structured outputs. This approach allows for extracting \n",
    "well-organized information from images in a machine-readable format, such as JSON.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': {'scene': 'A bustling city street with a mix of pedestrians and vehicles.',\n",
       "  'foreground': {'elements': [{'type': 'person',\n",
       "     'action': 'sitting',\n",
       "     'details': {'gender': 'male',\n",
       "      'age': 'teen',\n",
       "      'clothing': 'green jacket, shorts',\n",
       "      'activity': 'using a smartphone',\n",
       "      'position': 'on the ground'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'lying down',\n",
       "     'details': {'gender': 'male',\n",
       "      'age': 'teen',\n",
       "      'clothing': 'red hoodie',\n",
       "      'position': 'on the ground, appearing unconscious'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'sitting',\n",
       "     'details': {'gender': 'female',\n",
       "      'age': 'young adult',\n",
       "      'clothing': 'red top, blue jeans',\n",
       "      'activity': 'reading a book',\n",
       "      'position': 'on a bench'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'sitting',\n",
       "     'details': {'gender': 'male',\n",
       "      'age': 'older adult',\n",
       "      'clothing': 'suit',\n",
       "      'activity': 'reading a newspaper',\n",
       "      'position': 'on a bench'}},\n",
       "    {'type': 'person',\n",
       "     'action': 'walking',\n",
       "     'details': {'gender': 'female',\n",
       "      'age': 'young adult',\n",
       "      'clothing': 'pink top, shorts',\n",
       "      'activity': 'looking at her phone',\n",
       "      'position': 'walking past'}}],\n",
       "   'animals': [{'type': 'bird',\n",
       "     'species': 'pigeon',\n",
       "     'quantity': 'multiple',\n",
       "     'position': 'on the ground near the bench'}],\n",
       "   'plant': {'type': 'flower pot',\n",
       "    'details': {'flowers': 'geraniums',\n",
       "     'position': 'next to the sitting boy'}}},\n",
       "  'background': {'elements': [{'type': 'building',\n",
       "     'style': 'modern',\n",
       "     'details': {'windows': 'large, reflecting sunlight',\n",
       "      'position': 'on the left side'}},\n",
       "    {'type': 'building',\n",
       "     'style': 'historic',\n",
       "     'details': {'features': 'steeple, arched windows',\n",
       "      'position': 'in the background'}},\n",
       "    {'type': 'traffic light',\n",
       "     'state': 'yellow',\n",
       "     'position': 'above the street'},\n",
       "    {'type': 'vehicles',\n",
       "     'details': {'types': ['car', 'taxi', 'motorcycle'],\n",
       "      'action': 'moving',\n",
       "      'position': 'on the street'}}]},\n",
       "  'atmosphere': {'lighting': 'golden hour, warm tones',\n",
       "   'mood': 'busy, urban life'}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elements': [{'type': 'person',\n",
       "   'action': 'sitting',\n",
       "   'details': {'gender': 'male',\n",
       "    'age': 'teen',\n",
       "    'clothing': 'green jacket, shorts',\n",
       "    'activity': 'using a smartphone',\n",
       "    'position': 'on the ground'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'lying down',\n",
       "   'details': {'gender': 'male',\n",
       "    'age': 'teen',\n",
       "    'clothing': 'red hoodie',\n",
       "    'position': 'on the ground, appearing unconscious'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'sitting',\n",
       "   'details': {'gender': 'female',\n",
       "    'age': 'young adult',\n",
       "    'clothing': 'red top, blue jeans',\n",
       "    'activity': 'reading a book',\n",
       "    'position': 'on a bench'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'sitting',\n",
       "   'details': {'gender': 'male',\n",
       "    'age': 'older adult',\n",
       "    'clothing': 'suit',\n",
       "    'activity': 'reading a newspaper',\n",
       "    'position': 'on a bench'}},\n",
       "  {'type': 'person',\n",
       "   'action': 'walking',\n",
       "   'details': {'gender': 'female',\n",
       "    'age': 'young adult',\n",
       "    'clothing': 'pink top, shorts',\n",
       "    'activity': 'looking at her phone',\n",
       "    'position': 'walking past'}}],\n",
       " 'animals': [{'type': 'bird',\n",
       "   'species': 'pigeon',\n",
       "   'quantity': 'multiple',\n",
       "   'position': 'on the ground near the bench'}],\n",
       " 'plant': {'type': 'flower pot',\n",
       "  'details': {'flowers': 'geraniums', 'position': 'next to the sitting boy'}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"description\"][\"foreground\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# JSON Schema for Controlled Structured Outputs\n",
    "In this section, we define a JSON schema for a more controlled and specific output from the model. \n",
    "Using this schema, we can ensure the model adheres to predefined data types and structures while describing images.In this case we will provide an exmaple of json format answer, but ideally \n",
    "one could also do it via e.g. pydantic library.\n",
    "\n",
    "Example: \n",
    "```\n",
    "from typing import List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    position: str = Field(..., description=\"Position of the person in the environment, e.g., standing, sitting, etc.\")\n",
    "    age: int = Field(..., ge=0, description=\"Age of the person, must be a non-negative integer.\")\n",
    "    activity: str = Field(..., description=\"Activity the person is engaged in, e.g., reading, talking, etc.\")\n",
    "    gender: Literal[\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"] = Field(\n",
    "        ..., description=\"Gender of the person\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ImageExtraction(BaseModel):\n",
    "    number_of_people: int = Field(..., ge=0, description=\"The total number of people in the environment.\")\n",
    "    atmosphere: str = Field(..., description=\"Description of the atmosphere, e.g., calm, lively, etc.\")\n",
    "    hour_of_the_day: int = Field(..., ge=0, le=23, description=\"The hour of the day in 24-hour format.\")\n",
    "    people: List[Person] = Field(..., description=\"List of people and their details.\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]\n",
    "                    }}},\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_analysis = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alert service prompt \n",
    "\n",
    "alert_sys_prompt = \" you are an experienced first aid paramedical\"\n",
    "alert_prompt= \"\"\"Extract from the following scene analysis give to you in json format, \n",
    "if anyone might be in danger and if the Child Hospital or normal Hospital should be alerted. \n",
    "Give the a concise answer\n",
    "The situation is given to you from this object: \"\"\" + str(output_image_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this scene, the 16-year-old male who is lying down and unconscious is in danger. The normal hospital should be alerted for this situation, as it indicates a potential medical emergency.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt = alert_prompt, sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, there are no specific coordinates given for the individuals in the image. However, since the scenario describes a busy urban environment with various activities, we can infer that the 16-year-old male who is lying down and unconscious would likely be located in a more central area where people are gathered, possibly near a sidewalk or park bench.\\n\\nIn a busy urban setting, if we were to estimate coordinates, we might place the unconscious individual in a location that is easily accessible for emergency responders. For example, if we assume a grid layout for the area, we could suggest coordinates like (5, 5) or (10, 10) as a central point where the individual might be found.\\n\\nHowever, please note that these coordinates are purely hypothetical and should not be used for actual navigation or emergency response without proper visual confirmation. In a real-life scenario, it is crucial to assess the situation visually and ensure the safety of all individuals involved.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt = \"Considering the image analysis given\" +str(output_image_analysis)+ \"give me back the coordinates of the 16-years old. If these are not available, infer them form the pic\", sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[400, 600, 500, 700]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt =  \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\", sysprompt= alert_sys_prompt, image = encode_image(img)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Google VLM (Gemini)\n",
    "This section demonstrates the use of Google's Vision Language Model, Gemini. \n",
    "We explore basic text generation as well as its ability to analyze images and provide relevant outputs.\n",
    "\n",
    "**Support Material**:\n",
    "- https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from dotenv import load_dotenv  \n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv()\n",
    "#genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI works by combining large amounts of data with fast, iterative processing and intelligent algorithms, allowing the software to learn automatically from patterns or features in the data.  There's no single \"how it works\" because different AI approaches use different techniques.  However, here's a breakdown of common principles:\n",
      "\n",
      "**1. Data Acquisition and Preparation:**\n",
      "\n",
      "* **Data Collection:** AI systems need massive datasets relevant to the task they're designed for. This data could be anything from images and text to sensor readings and financial transactions.\n",
      "* **Data Cleaning and Preprocessing:**  Raw data is often messy and inconsistent. This stage involves removing errors, handling missing values, and transforming the data into a suitable format for the AI model.  This might include normalization, standardization, or feature extraction.\n",
      "\n",
      "**2. Model Selection and Training:**\n",
      "\n",
      "* **Algorithm Selection:**  The choice of algorithm depends on the type of problem being solved. Common approaches include:\n",
      "    * **Machine Learning (ML):**  This involves algorithms that learn from data without explicit programming.  Subcategories include:\n",
      "        * **Supervised Learning:** The algorithm is trained on labeled data (e.g., images tagged with the objects they contain).  Examples include linear regression, logistic regression, support vector machines (SVMs), decision trees, and neural networks.\n",
      "        * **Unsupervised Learning:** The algorithm is trained on unlabeled data and tries to find patterns or structures within the data.  Examples include clustering (k-means), dimensionality reduction (PCA), and association rule mining.\n",
      "        * **Reinforcement Learning:** The algorithm learns through trial and error by interacting with an environment and receiving rewards or penalties for its actions.  Examples include game playing AI and robotics control.\n",
      "    * **Deep Learning (DL):** A subfield of ML that uses artificial neural networks with multiple layers (hence \"deep\") to analyze data.  These networks can automatically learn complex features from raw data, making them powerful for tasks like image recognition, natural language processing, and speech recognition.\n",
      "* **Model Training:** This involves feeding the prepared data into the chosen algorithm.  The algorithm adjusts its internal parameters to minimize errors and improve its performance on the training data.  This process often involves complex mathematical optimization techniques.\n",
      "\n",
      "**3. Evaluation and Refinement:**\n",
      "\n",
      "* **Model Evaluation:** After training, the model's performance is evaluated on a separate dataset (the test set) that it hasn't seen before. This helps determine how well it generalizes to new, unseen data.  Metrics like accuracy, precision, recall, and F1-score are commonly used.\n",
      "* **Model Refinement:** If the model's performance isn't satisfactory, the process is iterated.  This may involve adjusting the algorithm's parameters, using more data, or trying a different algorithm altogether.\n",
      "\n",
      "**4. Deployment and Monitoring:**\n",
      "\n",
      "* **Deployment:** Once a satisfactory model is achieved, it's deployed into a real-world application, such as a website, mobile app, or embedded system.\n",
      "* **Monitoring:**  The model's performance is continuously monitored in the real world to detect any issues and ensure it continues to perform as expected.  This may involve retraining the model periodically with new data.\n",
      "\n",
      "\n",
      "In essence, AI mimics aspects of human intelligence by learning from data, identifying patterns, making predictions, and adapting to new information.  However, it's crucial to remember that AI systems are tools; their capabilities are limited by the data they are trained on and the algorithms they employ. They don't possess consciousness or genuine understanding in the human sense.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Explain how AI works\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[693,328,962,631]\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    im,\n",
    "    (\n",
    "        \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\\n \"\n",
    "    ),\n",
    "])\n",
    "response.resolve()\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini can be used to predict bounding boxes based on free form text queries.\n",
    "The model can be prompted to return the boxes in a variety of different formats (dictionary, list, etc). This of course migh need to be parsed. \n",
    "Check: https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing#scrollTo=WFLDgSztv77H\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
